{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenCVでとことん画像処理 - 後半\n",
    "このノートは **後半** のノートになります。後半では\n",
    "\n",
    "- [空間フィルタリング](#spatial)\n",
    "\t- [平滑化](#smoothing)\n",
    "\t\t- [平均化フィルタ](#mean)\n",
    "\t\t- [加重平均化(ガウシアン)フィルタ](#weight_mean)\n",
    "\t\t- [メディアンフィルタ](#median)\n",
    "\t- [エッジ抽出](#edge)\n",
    "\t\t- [Cannyアルゴリズムによるエッジ検出](#canny)\n",
    "\t- [鮮鋭化(アンシャープマスキング)](#sharpening)\n",
    "\t- [エッジを保存した平滑化](#edge_smoothing)\n",
    "\t\t- [バイラテラルフィルタ](#bilateral)\n",
    "\t\t- [ノンローカルミーンフィルタ](#non_local_mean)\n",
    "- [二値化](#binarize)\n",
    "\t- [単純なしきい値処理](#threshold)\n",
    "\t- [複雑なしきい値処理](#complex_threshold)\n",
    "- [領域分割](#area_separate)\n",
    "    - [クロマキーによるマスク処理](#chromakey)\n",
    "        - [画像合成](#composition)\n",
    "    - [ミーンシフト法](#mean_shift)\n",
    "    - [grabcut法](#grabcut)\n",
    "- [まとめ](#conclusion)\n",
    "    \n",
    "を扱います。\n",
    "\n",
    "※追記: Python 3.8.2, chromium ベースのブラウザで動作確認してあります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モジュールのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# これを書くとjupyterでmatplotlibなどが出力する画像の解像度が上がる\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import time\n",
    "\n",
    "from utils.plot import compare_plot\n",
    "\n",
    "print(cv2.__version__)\n",
    "print(matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"basic\"></a>\n",
    "## 画像の読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.cvtColor(cv2.imread(\"../../data/Lenna_flat.png\"), cv2.COLOR_BGR2RGB)\n",
    "img_cat = cv2.cvtColor(cv2.imread(\"../../data/kuroneko.png\"), cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上のRGB画像をグレー画像にして読み込みます\n",
    "img_gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "plt.imshow(img_gray, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"spatial\"></a>\n",
    "## 空間フィルタリング\n",
    "前半では画素ごとの濃淡変換を対応する**画素値**のみで行っていたが、画素値のみではなくその周辺の画素も含めた領域内の画素値を元に計算します。この処理を**空間フィルタリング**と呼び、そこで用いるフィルタは一般的に空間フィルタと呼びます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"smoothing\"></a>\n",
    "## 平滑化\n",
    "画像になめらかな濃淡変換を与える処理を**平滑化**と呼んでおり、画像に含まれるノイズなどの不要な濃淡変動を軽減するときなどに用いられます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"mean\"></a>\n",
    "### 平均化フィルタ\n",
    "画像をぼかすためにフィルタのサイズを元に平均をとったフィルタを用いて画像をぼかす手法を**平均化フィルタ**と呼びます。\n",
    "\n",
    "<img src=\"../../fig/mean_filter.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 手動でフィルタを定義する場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# フィルタの定義\n",
    "size = 5\n",
    "kernel = np.ones((size,size),np.float32) / size**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_mean_sm = cv2.filter2D(img, -1, kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 平均化フィルタを関数で実行する場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_blur = cv2.blur(img, (size, size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_plot([img, img_blur], [\"before\", \"after\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"weight_mean\"></a>\n",
    "### 加重平均化(ガウシアンフィルタ)\n",
    "単純なフィルターサイズに基づいた平均値ではなく、中央に近いほど大きな重みをつける**加重平均化フィルタ**というものもあります。その重みの中でも**ガウス分布(標準正規分布)**に近づけたものを**ガウシアンフィルタ**と呼びます。\n",
    "\n",
    "標準正規分布を2次元(画像)に拡張した分布は以下のように表されます。\n",
    "\n",
    "$$ f(x) =  \\frac{1}{ 2 \\pi \\sigma^2 } \\exp \\left( -  \\frac{x^2 + y^2}{2 \\sigma^2} \\right) $$\n",
    "\n",
    "<table border=\"0\">\n",
    "<tr>\n",
    "<td><img src=\"../../fig/two_dim_gausian.png\" width=300></td>\n",
    "<td><img src=\"../../fig/weight_mean_filter.png\" width=300></td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_g_blur = cv2.GaussianBlur(img,(5,5),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_plot([img, img_g_blur], [\"before\", \"after\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単純な平均化フィルタに比べて見た目にあまり大きな違いはありませんが、よりなめらかで自然な平滑化の効果を期待することができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"median\"></a>\n",
    "### メディアンフィルタ\n",
    "メディアンフィルタはフィルタサイズで指定された範囲の中の中央値を元の値と入れ替えるというもので、特にソルト・ペッパーノイズ(インパルスノイズ)の除去に強いと言われている手法です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前半で作成したソルト・ペッパーノイズ画像を読み込む\n",
    "img_sp = cv2.imread(\"../../data/img_sp.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 5\n",
    "img_blur = cv2.blur(img_sp, (size, size))\n",
    "img_g_blur = cv2.GaussianBlur(img_sp, (size,size), 0)\n",
    "img_m_blur = cv2.medianBlur(img_sp, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_plot([img_sp, img_blur, img_g_blur, img_m_blur], \\\n",
    "            [\"salt&pepper img\", \"mean_filter\", \"gaussian_mean_filter\", \"median_filter\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"edge\"></a>\n",
    "## エッジ抽出\n",
    "画像中で明るさが急激に変化する**エッジ部分**を取り出す手法をエッジ抽出と呼びます。これらの処理は画像中の特徴や図形を検出するための前処理として用いられることが多いです。\n",
    "\n",
    "またエッジ抽出には微分フィルタやソーベルフィルタ、ラプラシアンフィルタなど様々な手法が存在しますが、その中でも輪郭の検出漏れや誤検出が少ない手法であ**Cannyアルゴリズム**について説明します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"canny\"></a>\n",
    "### Cannyアルゴリズムによるエッジ抽出\n",
    "Cannyアルゴリズムでは大まかな流れとして、\n",
    "\n",
    "1. ノイズの除去\n",
    "2. 輪郭の抽出\n",
    "3. 非極大抑制\n",
    "4. ヒステリシスしきい値処理\n",
    "\n",
    "の4つで処理を行っていきます。詳しい処理についてはここでは時間の都合で説明を割愛させていただきますが、わかりやすく詳しい解説をノート最下部の参考サイトに乗せていますのでそちらを参照ください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_canny = cv2.Canny(img_gray,100,200) #引数は 画像,最小閾値,最大閾値"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_plot([img_gray, img_canny], [\"before\", \"after\"], [\"gray\", \"gray\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sharpening\"></a>\n",
    "## 鮮鋭化\n",
    "エッジ抽出はエッジのみの抽出でしたが、鮮鋭化という手法では元の画像の濃淡を残したままエッジを強調します。元の画像に対して平滑化処理をした上で、平滑化した画像から元の画像を引くことでエッジが得られ、この差の画像を元画像に足し合わせることで、エッジが強調された画像を得ることができます。このような処理を**アンシャープマスキング**と呼んでいます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_cp1 = img.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unsharp_masking(img, i):\n",
    "    img_gblur = cv2.GaussianBlur(img, (i,i), 10.0)\n",
    "    unsharp_image = cv2.addWeighted(img, 1.5, img_gblur, -0.5, 0, img)\n",
    "    return unsharp_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 複数回実行で重ねがけできます\n",
    "compare_plot([img, unsharp_masking(img_cp1, 9)], \\\n",
    "             [\"img\", \"img + edge\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_cp2 = img_cat.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_plot([img_cat, unsharp_masking(img_cp2, 9)], \\\n",
    "             [\"img\", \"img + edge\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "エッジが強調され、より鮮鋭化の度合いが増していることが比較してわかります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"edge_smoothing\"></a>\n",
    "## エッジを保存した平滑化\n",
    "これまで紹介した単純な平滑化では、画像に含まれるノイズなどの濃淡変動を軽減できていますが、それと同時に画像にもともとあるエッジもなめらかにしてしまうという問題がありました。そこで平滑化と鮮鋭化の2つを満たすような平滑化の手法が提案されており、今回はその中でも**バイラテラルフィルタ**と**ノンローカルミーンフィルタ**の2つを紹介していきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"birateral\"></a>\n",
    "### バイラテラルフィルタ\n",
    "平滑化アルゴリズムの一つであるガウシアンフィルタでは注目画素と周辺がその距離にガウス分布で近似した重みをかけることで平滑化していましたが、バイラテラルフィルタではそれに加えて**注目画素と周辺画素の画素値の差**についてもガウス分布で重み付けをしています。\n",
    "\n",
    "大雑把に説明すると、カーネルの中心の輝度値と差の少ないところだけをガウシアンフィルタで平滑化することで、近い色はぼかして、遠い色は際立たせることができます。輪郭付近の重みを0か1にするかは正規分布で決めていくことで差が小さいと重みが大きく、輝度差が大きいと重みが小さくなるように、なだらかに変化します。\n",
    "\n",
    "<img src=\"../../fig/birateral.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_cp1 = img.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def birateral_filter(img, n):\n",
    "    \"\"\"\n",
    "    オプションについて\n",
    "    \n",
    "    cv2.bilateralFilter(img, d, sigmaColor, sigmaSpace, \n",
    "                                dst, borderType)\n",
    "    \n",
    "    img ： 入力画像\n",
    "    d ： 注目画素をぼかすために使う領域\n",
    "    sigmaColor： 色についての標準偏差(大きいと画素の差が大きくても大きな重みを使う)\n",
    "    sigmaSpace：色についての標準偏差(大きいと画素の差が大きくても大きな重みを使う)\n",
    "    \"\"\"\n",
    "    for _ in range(n):\n",
    "        img = cv2.bilateralFilter(img, 9, 75, 75)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# バイラテラルフィルタを複数かけて比較\n",
    "compare_plot([img] + [birateral_filter(img_cp1, i) for i in range(1,5)], \\\n",
    "             [\"before\"] + [\"birateral_{}\".format(i) for i in range(1,5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"non_local_mean\"></a>\n",
    "### ノンローカルミーンフィルタ\n",
    "ノンローカルミーンフィルタはテンプレートマッチングのように周辺画素を含めた領域が、注目がその周辺領域にどれだけ似ているかで重みを決めていきます。\n",
    "\n",
    "<img src=\"../../fig/nlmf.png\" width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_local_mean_filter(img, n):\n",
    "    \"\"\"\n",
    "    オプションについて\n",
    "    \n",
    "    cv2.fastNlMeansDenoisingColored(img, dst, h, hColor, \n",
    "                                templateWindowSize, searchWindowSize)\n",
    "    \n",
    "    img ： 入力画像\n",
    "    dst ： 出力画像\n",
    "    h ： 輝度成分のフィルタの平滑化の度合い\n",
    "    hColor： 色成分の不フィルタの平滑化の度合い\n",
    "    templateWindowSize：周辺領域のテンプレートサイズ\n",
    "    searchWindowSize：重みを探索する領域サイズ\n",
    "    \"\"\"\n",
    "    for _ in range(n):\n",
    "        img = cv2.fastNlMeansDenoisingColored(img,None,10,10,7,21)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_cp1 = img.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ノンローカルミーンフィルタを複数かけて比較\n",
    "compare_plot([img] + [non_local_mean_filter(img_cp1, i) for i in range(1,5)], \\\n",
    "             [\"before\"] + [\"non_local_mean_filter_{}\".format(i) for i in range(1,5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記の2つに示されるように、画像をぼやかしながらなおかつエッジが残ったシャープな画像を作る際にバイラテラルフィルタとノンローカルミーンフィルタは最適な処理になっています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"binarize\"></a>\n",
    "## 2値化\n",
    "前半で少しだけ2値化について触れましたが、2値化して得たグレースケール画像を用いた画像処理は画像解析において多くの成果を上げてきています。その2値化処理についてより深く掘り下げていきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"threshold\"></a>\n",
    "## 単純なしきい値処理\n",
    "2値画像では、ある画素以上の画素値を1、それ未満の値を0としたときにその境界の値を**しきい値**と呼び、文字認識を行うような場合に必要となる処理です。\n",
    "\n",
    "OpenCVでは2値化する形式を複数選ぶことができ、それらを形式ごとにどういう処理になっているかを確認していきたいと思います\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "img : 画像\n",
    "thresh : しきい値\n",
    "max_value : 画素の最大値\n",
    "2値化するためのタイプ : 後述\n",
    "\"\"\"\n",
    "\n",
    "# OpenCVでの2値化に使用する関数\n",
    "cv2.threshold(img, thresh, max_value, type)\n",
    "```\n",
    "\n",
    "+ THRESH_BINARY\n",
    "    + しきい値を元に白と黒に2値化します\n",
    "    \n",
    "$$ {\\displaystyle dst(x, y) = \\left\\{ \\begin{array}{l} maxValue & (src(x, y) > threshold) \\\\ 0 & (otherwise) \\end{array} \\right. } $$\n",
    "\n",
    "+ THRESH_BINARY_INV\n",
    "    + THRESH_BINARYを逆にしたものです\n",
    "    \n",
    "$$ {\\displaystyle dst(x, y) = \\left\\{ \\begin{array}{l} 0 & (src(x, y) > threshold) \\\\ maxValue & (otherwise) \\end{array} \\right. } $$\n",
    "\n",
    "+ THRESH_TRUNC\n",
    "    + しきい値を超えていればしきい値を画素値にし、そうでない場合はそのままです\n",
    "    \n",
    "$$ {\\displaystyle dst(x, y) = \\left\\{ \\begin{array}{l} threshhold & (src(x, y) > threshold) \\\\ src(x,y) & (otherwise) \\end{array} \\right. } $$\n",
    "\n",
    "+ THRESH_TOZERO\n",
    "    + しきい値を超えていればそのままで、そうでない値を黒にします\n",
    "\n",
    "$$ {\\displaystyle dst(x, y) = \\left\\{ \\begin{array}{l} src(x,y) & (src(x, y) > threshold) \\\\ 0 & (otherwise) \\end{array} \\right. } $$\n",
    "\n",
    "+ THRESH_TOZERO_INV\n",
    "    + THRESH_TOZERO_INVを逆にしたものです\n",
    "\n",
    "$$ {\\displaystyle dst(x, y) = \\left\\{ \\begin{array}{l} 0 & (src(x, y) > threshold) \\\\ src(x,y) & (otherwise) \\end{array} \\right. } $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,thresh1 = cv2.threshold(img_gray,127,255,cv2.THRESH_BINARY)\n",
    "_,thresh2 = cv2.threshold(img_gray,127,255,cv2.THRESH_BINARY_INV)\n",
    "_,thresh3 = cv2.threshold(img_gray,127,255,cv2.THRESH_TRUNC)\n",
    "_,thresh4 = cv2.threshold(img_gray,127,255,cv2.THRESH_TOZERO)\n",
    "_,thresh5 = cv2.threshold(img_gray,127,255,cv2.THRESH_TOZERO_INV)\n",
    "_,thresh6 = cv2.threshold(img_gray,0,255,cv2.THRESH_BINARY + cv2.THRESH_OTSU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_plot([img_gray, thresh1, thresh2, thresh3, thresh4, thresh5, thresh6], \\\n",
    "            [\"gray\", \"binary\", \"binary_inv\", \"trunc\", \"to_zero\", \"to_zero_inv\", \"otsu\"],\n",
    "            [\"gray\" for _ in range(7)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"complex_threshold\"></a>\n",
    "## 複雑なしきい値処理\n",
    "これまでの単純な2値化と違い、様々な条件下において2値化処理を行っていきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"adaptive\"></a>\n",
    "### 適応的しきい値処理\n",
    "画像の光源環境によって明るさが異なるような画像を2値化する際に**適応的しきい値処理**を行います。適応的しきい値処理では、画像中の小領域ごとにしきい値を計算するため、単純なしきい値処理よりも良い結果を得られるのが特徴です。\n",
    "\n",
    "+ cv2.adaptiveThreshold(img, max_value, adaptive_thresh, threshhold, blocksize, c)\n",
    "    + adaptive_thresh\n",
    "        + 小領域内のしきい値の計算方法\n",
    "            + ADAPTIVE_THRESH_MEAN_C:近傍領域の中央値をしきい値\n",
    "            + ADAPTIVE_THRESH_GAUSSIAN_C:近傍領域の重み付け平均値をしきい値\n",
    "    + blocksize\n",
    "        + しきい値計算に使用する近傍領域のサイズ(blocksize > 1)\n",
    "    + c\n",
    "        + 計算されたしきい値から引く定数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 適応的しきい値処理をするために一度メディアンフィルタをかけます\n",
    "img_med = cv2.medianBlur(img_gray, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adap1 = cv2.adaptiveThreshold(img_med, 255, cv2.ADAPTIVE_THRESH_MEAN_C, \\\n",
    "                            cv2.THRESH_BINARY, 11, 2)\n",
    "adap2 = cv2.adaptiveThreshold(img_med, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \\\n",
    "                            cv2.THRESH_BINARY, 11, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_plot([img_med, thresh1, adap1, adap2], \\\n",
    "            [\"median\", \"thresh_binary\", \"thresh_mean\", \"thresh_gaussian\"], \\\n",
    "            [\"gray\" for _ in range(4)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"area_separate\"></a>\n",
    "## 領域分割\n",
    "画像に写っている対象を構成する領域を取り出して解析したい場合に行う処理が**領域分割**になります。そのなかでも代表的な手法を中心に紹介していきたいと思います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"chromakey\"></a>\n",
    "### クロマキーによるマスク処理\n",
    "クロマキーとはある特定の色を選択してそれを背景とみなすことで、領域分割を行う手法です。主にクロマキー処理では領域分割後に画像を合成するような処理に応用されます。任意の2枚の画像を合成するまでを一貫してやっていきましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_chroma = cv2.cvtColor(cv2.imread(\"../../data/kuroneko_chromakey.png\"), cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_chroma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 今回は画素値[0,0,255]を対象にクロマキーで分割していく\n",
    "img_chroma[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# クロマキーで抽出する画素をRGBごとに照らし合わせます\n",
    "img_mask = (img_chroma == [0,0,255])\n",
    "img_mask = img_mask.astype('uint8') * 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# マスクをグレースケールにして2値化してmedianfilterをかけて粗を取ります\n",
    "img_mask_g1 = cv2.cvtColor(img_mask, cv2.COLOR_RGB2GRAY)\n",
    "img_mask_g2 = cv2.threshold(img_mask_g1, 254, 255, cv2.THRESH_BINARY_INV)[1]\n",
    "img_mask_g = cv2.medianBlur(img_mask_g2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_chromakey = cv2.bitwise_and(img_chroma, cv2.cvtColor(img_mask_g, cv2.COLOR_GRAY2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_plot([img_chroma, img_mask, img_mask_g1, img_mask_g2, img_mask_g, img_chromakey], \\\n",
    "            [\"original\", \"mask(RGB)\", \"mask(Gray)\", \"mask(threshold)\", \"mask(medianBlur)\", \"chromakey\"], \\\n",
    "            [None, None, \"gray\", \"gray\", \"gray\", None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"composition\"></a>\n",
    "### 画像合成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pydata = cv2.cvtColor(cv2.imread(\"../../data/pydataokinawa.png\"), cv2.COLOR_BGR2RGB)\n",
    "img_pydata = cv2.resize(img_pydata, img_chromakey.shape[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pydata画像からcat画像をくり抜きます\n",
    "img_maskn = cv2.bitwise_not(img_mask_g)\n",
    "img_pydata_ext = cv2.bitwise_and(img_pydata, cv2.cvtColor(img_maskn, cv2.COLOR_GRAY2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dst = cv2.bitwise_or(img_pydata_ext, img_chromakey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_plot([img_pydata, img_chromakey, img_pydata_ext, img_dst], \\\n",
    "            [\"original\", \"chromakey\", \"extract\", \"composition\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"mean_shift\"></a>\n",
    "### ミーンシフト法\n",
    "ミーンシフト法は画像領域の分割や対象画像の追跡に用いられる手法です。ここでは色の減算にミーンシフトを用いることで領域を分割しやすくしています。\n",
    "\n",
    "ミーンシフトではある画素に対して、その点の中心となる半径の円を考えて、その重心を求めていきます。その重心を中心にという形で重心の極大値を求めていくと似た色が集まってくる(領域が分割される)仕組みになっています。\n",
    "\n",
    "<img src=\"../../fig/meanshift.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "cv2.pyrMeanShiftFiltering(img, sp, sr)\n",
    "\n",
    "sp:円の半径\n",
    "sr:画素値の範囲\n",
    "\"\"\"\n",
    "\n",
    "img_ms = cv2.pyrMeanShiftFiltering(img, 30, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_ms = cv2.pyrMeanShiftFiltering(img_ms, 30, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 境界線の抽出\n",
    "img_ms_gray = cv2.cvtColor(img_ms, cv2.COLOR_RGB2GRAY)\n",
    "edge = cv2.bilateralFilter(img_gray, 9, 75, 75)\n",
    "edge = cv2.Canny(edge, 50, 150, apertureSize=3)\n",
    "edge = cv2.cvtColor(edge, cv2.COLOR_GRAY2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dst = cv2.subtract(img_ms, edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_plot([img, img_ms, edge, img_dst], \\\n",
    "             [\"original\", \"mean_shift\", \"edge\", \"subtract\"], \\\n",
    "            [None, None, \"gray\", \"subtract\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"grabcut\"></a>\n",
    "## grabcut法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まず前景と背景の初期境界(rect)を大まかに指定し、次にそれぞれの色分布を基にグラフカットで前景と背景の境界を求めます。この新しい境界から求めた前景と背景の色分布計算と Graph Cut による境界算出を繰り返し行うことで、前景と背景を求めます。\n",
    "\n",
    "アルゴリズムは数学的に扱うとかなり難しいのでここでは割愛しますが、このgrabcut法とマスクの手入力によって1枚における処理の時間を減らそうという手法です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "画像の領域で見てみると\n",
    "+ x軸はだいたい50から400前後\n",
    "+ y軸はだいたい50から512まで\n",
    "\n",
    "にLennaさんがいるのでこれらを対象に矩形を取っていきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rect = (50, 50, 420, 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次にマスク用の領域を作っていきます。grabcutではあとから人間の手で前景と背景を選択できるので画像処理がうまく行かなかったときに修正するように作成していきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.zeros(img.shape[:2], np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前景と背景のGMMモデルを作成します\n",
    "bgModel = np.zeros((1, 65), np.float64)\n",
    "fgModel = np.zeros((1, 65), np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.grabCut(img, mask, rect, bgModel, fgModel, 5, cv2.GC_INIT_WITH_RECT)\n",
    "# maskの値が1の部分のみ残して0をクリアする\n",
    "mask2 = np.where((mask == 2) | (mask == 0), 0,1).astype(np.uint8)\n",
    "img_grab = img * mask2[:,:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_grab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "帽子の一部が矩形として取得されなかったので、この部分を前景として修正していきましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML('''\n",
    "<canvas id=\"canvas\" height=\"512px\" width=\"512px\" style=\"border: 1px solid;background: url('../../data/Lenna_flat.png')\"></canvas>\n",
    "<p>\n",
    "    <button id=\"clear\">クリア</button>\n",
    "    <button id=\"white\">前景(白色)</button>\n",
    "    <button id=\"black\">背景(黒色)</button>\n",
    "    <button id=\"submit\">画像のbase64化</button>\n",
    "</p>\n",
    "<p id=\"msg\"></p>\n",
    "<script>\n",
    "    var kernel = IPython.notebook.kernel;\n",
    "\n",
    "    var config = {\n",
    "        \"linesize\": 12,\n",
    "        \"linecolor\": \"#000000\"\n",
    "    }\n",
    "\n",
    "    var mouse = {\n",
    "        \"X\": null,\n",
    "        \"Y\": null,\n",
    "    }\n",
    "\n",
    "    var clear = document.getElementById(\"clear\");\n",
    "    var submit = document.getElementById(\"submit\");\n",
    "    var white = document.getElementById(\"white\");\n",
    "    var black = document.getElementById(\"black\");\n",
    "    var canvas = document.getElementById(\"canvas\");\n",
    "    var ctx = canvas.getContext(\"2d\");\n",
    "\n",
    "    ctx.strokeStyle = config.linecolor;\n",
    "    \n",
    "    clear.addEventListener(\"click\", function(){\n",
    "        ctx.clearRect(0, 0, canvas.width, canvas.height);\n",
    "    });\n",
    "\n",
    "    submit.addEventListener(\"click\", function(){\n",
    "        var variable_value = 'img_base64';\n",
    "        kernel.execute(variable_value + \" = '\" + canvas.toDataURL() + \"'\");\n",
    "        msg.textContent = \"Success: \" + \"image -> \" + variable_value;\n",
    "    });\n",
    "    \n",
    "    white.addEventListener(\"click\", function(){\n",
    "        ctx.strokeStyle = \"#FFFFFF\";\n",
    "    });\n",
    "    \n",
    "    black.addEventListener(\"click\", function(){\n",
    "        ctx.strokeStyle = \"#000000\";\n",
    "    });\n",
    "\n",
    "    canvas.addEventListener(\"mouseup\", drawEnd, false);\n",
    "    canvas.addEventListener(\"mouseout\", drawEnd, false);\n",
    "    \n",
    "    canvas.addEventListener(\"mousemove\", function(e){\n",
    "        if (e.buttons === 1 || e.witch === 1) {\n",
    "            var rect = e.target.getBoundingClientRect();\n",
    "            var X = e.clientX - rect.left;\n",
    "            var Y = e.clientY - rect.top;\n",
    "            draw(X, Y);\n",
    "        };\n",
    "    });\n",
    " \n",
    "    canvas.addEventListener(\"mousedown\", function(e){\n",
    "        if (e.button === 0) {\n",
    "            var rect = e.target.getBoundingClientRect();\n",
    "            var X = e.clientX - rect.left;\n",
    "            var Y = e.clientY - rect.top;\n",
    "            draw(X, Y);\n",
    "        }\n",
    "    });\n",
    "\n",
    "    function draw(X, Y) {\n",
    "        ctx.beginPath();\n",
    "        if (mouse.X === null) {\n",
    "            ctx.moveTo(X, Y);\n",
    "        } else {\n",
    "            ctx.moveTo(mouse.X, mouse.Y);\n",
    "        }\n",
    "        ctx.lineTo(X, Y);\n",
    "        \n",
    "        ctx.lineCap = \"round\";\n",
    "        ctx.lineWidth = config.linesize * 2;\n",
    "        \n",
    "        ctx.stroke();\n",
    "\n",
    "        mouse.X = X;\n",
    "        mouse.Y = Y;\n",
    "    };\n",
    " \n",
    "    function drawEnd() {\n",
    "        mouse.X = null;\n",
    "        mouse.Y = null;\n",
    "    }\n",
    "</script>\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import base64\n",
    "\n",
    "img_base64 = img_base64.split(\",\")[-1]\n",
    "img_rect = np.array(Image.open(BytesIO(base64.b64decode(img_base64))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rect = cv2.cvtColor(img_rect, cv2.COLOR_RGBA2GRAY) // 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask3 = cv2.bitwise_or(mask2, img_rect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_plot([img_rect, mask2, mask3], \\\n",
    "            [\"manual_mask\", \"grabcut_mask\", \"bitwise_or\"],\n",
    "            [\"gray\" for _ in range(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.grabCut(img, mask3, None, bgModel, fgModel, 5, cv2.GC_INIT_WITH_MASK)\n",
    "mask = np.where((mask3 == 2) | (mask3 == 0), 0, 1).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_grab2 = img * mask[:, :, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_grab2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grabcut法と手入力によるマスクによってうまくLennaさんだけを抽出することができました！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conclusion\"></a>\n",
    "## まとめ\n",
    "今回の画像処理ハンズオンにて紹介した処理は、画像処理の中のほんの少しでしかなく、これら以外にも様々な処理が存在しています。また最近のニューラルネットによる技術の向上によりGANなどによる画像生成など画像処理分野はより多種多様になっています。これをきっかけに画像処理分野により興味を持っていただけたらと思います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考資料・サイト\n",
    "+ ディジタル画像処理 改訂新版 5,9,10章\n",
    "+ Canny edge detection\n",
    "> https://imagingsolution.net/imaging/canny-edge-detector/\n",
    "\n",
    "+ バイラテラルフィルタ\n",
    "> https://imagingsolution.net/imaging/bilateralfilter/\n",
    "\n",
    "+ ノンローカルミーンフィルタ\n",
    "> http://opencv.jp/opencv2-x-samples/non-local-means-filter\n",
    "\n",
    "+ MeanShiftを用いたImage Segmentation\n",
    "> https://news.mynavi.jp/article/cv_future-35/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
